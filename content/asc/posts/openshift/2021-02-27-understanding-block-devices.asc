{:title "Understanding block device handling in OpenShift and some notes on CSI"
 :description "A basic introduction into block device usage "
 :layout
 :post
 :tags ["Storage", "OpenShift", "OCP", "Block devices"]
 :toc false
 :author "Toni Schmidbauer"
}


// Asciidoc Parameters
// toc is set here since I like unnumbered tocs more
:icons: font
:linkattrs:
:toc: macro

[.small]
_Last Modified: {docdatetime}_
// Asciidoc Parameters END

In this blog post we would like to explore OpenShift / Kubernetes
block device handling. We try to answer the following questions:

* What happens if multiple pods try to access the same block device?
* What happens if we scale a deployment using block devices to more than one replica?

And finally we want to give a short, high level overview about how the
container storage interface (CSI) actually works.

= Test setup

For running our test we need the following resources

* A new namespace/project for running our tests
* A persitent volume claim (PVC) to be mounted in our test pods
* Two pods definitions for mounting the PVC

== Step 1: Creating a new namespace/project

To run our test cases we created a new project with openshift

[source,bash]
----------
oc new-project blockdevices
----------

== Step 2: Defining a block PVC

Our cluster is running the rook operator (https://rook.io[]) and provides a ceph-block
storage class for creating block devices:

[source,bash]
----------
$ oc get sc
NAME                 PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
ceph-block           rook-ceph.rbd.csi.ceph.com     Delete          Immediate              false                  4d14h
----------

Let's take a look a the details of the storage class:

[source,yaml]
----------
$ oc get sc -o yaml ceph-block
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-block
parameters:
  clusterID: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  imageFeatures: layering
  imageFormat: "2"
  pool: blockpool
provisioner: rook-ceph.rbd.csi.ceph.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
----------

So whenever we create a PVC using this storage class the ceph
provisioner will also create an EXT4 filesystem on the block device.

To test block device handling we create the following persistent volume claim (PVC):

[source,yaml]
----------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: ceph-block
----------

The access mode is set to ReadWriteOnce (RWO), as block devices

[source,bash]
----------
oc create -f pvc.yaml
----------

[source,bash]
----------
$ oc get pvc
NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
block-claim   Bound    pvc-bd68be5d-c312-4c31-86a8-63a0c22de844   1Gi        RWO            ceph-block     91s
----------

For tests mounting the block device in a pod we use the following two pod definitions

[source,yaml]
----------
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: block-pod-a
  name: block-pod-a
spec:
  containers:
  - image: registry.redhat.io/ubi8/ubi:8.3
    name: block-pod-a
    command:
      - sh
      - -c
      - 'df -h /block && findmnt /block && sleep infinity'
    volumeMounts:
    - name: blockdevice
      mountPath: /block
  volumes:
  - name: blockdevice
    persistentVolumeClaim:
      claimName: block-claim
----------

[source,yaml]
----------
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: block-pod-b
  name: block-pod-b
spec:
  containers:
  - image: registry.redhat.io/ubi8/ubi:8.3
    name: block-pod-b
    command:
      - sh
      - -c
      - 'df -h /block && findmnt /block && sleep infinity'
    volumeMounts:
    - name: blockdevice
      mountPath: /block
  volumes:
  - name: blockdevice
    persistentVolumeClaim:
      claimName: block-claim
----------

[source,bash]
----------
10s         Warning   FailedAttachVolume       pod/block-pod-b                     Multi-Attach error for volume "pvc-bd68be5d-c312-4c31-86a8-63a0c22de844" Volume is already used by pod(s) block-pod-a
----------

CSI
===
https://kubernetes-csi.github.io/docs/

Kubelet CSI driver communication:

Container Storage Interface is registered to each kublet. When csi driver registers itself with the kublet.
Kubelet communicates with CSI driver via Unix Domain Socket. Socket is discovered upon registration.
