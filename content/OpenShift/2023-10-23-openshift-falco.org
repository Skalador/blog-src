#+title: Running Falco on OpenShift 4.12
#+author: Toni Schmidbauer
#+lastmod: [2023-10-23 Mon 11:30]
#+categories[]: OpenShift
#+draft: false
#+variable: value
#+date: 2023-10-23
#+list[]: value_1 value_2 value_3

[[https://falco.org/][Falco]] is a security tool to monitor kernel events like system calls to
provide real-time alerts.

In this post I'll document the steps taken to get Open Source [[https://falco.org/][Falco]]
running on an OpenShift 4.12 cluster.

# more


* First Try

We will use the [[https://falcosecurity.github.io/charts][Falco Helm chart]] version 3.8.0 for our first try of setting up Falco.

This is our values file:

#+begin_src
driver:
  kind: ebpf

falco:
  json_output: true
  json_include_output_property: true
  log_syslog: false
  log_level: info

falcosidekick:
  enabled: true
  webui:
    enabled: true
#+end_src

Because of reasons we leverage kustomize to render the helm chart. The
final kustomize config is [[https://github.com/tosmi-gitops/openshift-gitops/tree/main/components/apps/falco/base][here]].

So after deploying the chart via ArgoCD (another story), we have the following pods:

#+begin_src bash
$ oc get pods -n falco
NAME                             READY   STATUS                  RESTARTS      AGE
falco-4cc8j                      0/2     Init:CrashLoopBackOff   5 (95s ago)   4m31s
falco-bx87j                      0/2     Init:CrashLoopBackOff   5 (75s ago)   4m29s
falco-ds9w6                      0/2     Init:CrashLoopBackOff   5 (99s ago)   4m30s
falco-falcosidekick-ui-redis-0   1/1     Running                 0             4m28s
falco-gxznz                      0/2     Init:CrashLoopBackOff   3 (14s ago)   4m30s
falco-vtnk5                      0/2     Init:CrashLoopBackOff   5 (98s ago)   4m29s
falco-wbn2k                      0/2     Init:CrashLoopBackOff   5 (80s ago)   4m29s
#+end_src

hm, not so good. Seems like some ~initContainers~ are failing.

Let's check the ~falco-driver-loader~ initContainer:

#+begin_src bash
* Setting up /usr/src links from host
* Running falco-driver-loader for: falco version=0.36.1, driver version=6.0.1+driver, arch=x86_64, kernel release=4.18.0-372.73.1.el8_6.x86_64, kernel version=1
* Running falco-driver-loader with: driver=bpf, compile=yes, download=yes
* Mounting debugfs
mount: /sys/kernel/debug: permission denied.
       dmesg(1) may have more information after failed mount system call.
* Filename 'falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o' is composed of:
 - driver name: falco
 - target identifier: rhcos
 - kernel release: 4.18.0-372.73.1.el8_6.x86_64
 - kernel version: 1
* Trying to download a prebuilt eBPF probe from https://download.falco.org/driver/6.0.1%2Bdriver/x86_64/falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o
curl: (22) The requested URL returned error: 404
Unable to find a prebuilt falco eBPF probe
* Trying to compile the eBPF probe (falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o)
expr: syntax error: unexpected argument '1'
make[1]: *** /lib/modules/4.18.0-372.73.1.el8_6.x86_64/build: No such file or directory.  Stop.
make: *** [Makefile:38: all] Error 2
mv: cannot stat '/usr/src/falco-6.0.1+driver/bpf/probe.o': No such file or directory
Unable to load the falco eBPF probe
#+end_src

And we told the helm chart to enable ~falco-sidekick~ and
~falco-sidekick-ui~, but where are they?

Let's check the events with ~oc get events~ as well, and what do we see?

#+begin_src shell
8s          Warning   FailedCreate             replicaset/falco-falcosidekick-7cfbbbf89f      Error creating: pods "falco-falcosidekick-7cfbbbf89f-" is forbidden: unable to validate against any security context constraint: [provider "anyuid": Forbidden: not usable by user or serviceaccount, provider restricted-v2: .spec.securityContext.fsGroup: Invalid value: []int64{1234}: 1234 is not an allowed group, provider restricted-v2: .containers[0].runAsUser: Invalid value: 1234: must be in the ranges: [1000730000, 1000739999], provider "restricted": Forbidden: not usable by user or serviceaccount, provider "nonroot-v2": Forbidden: not usable by user or serviceaccount, provider "nonroot": Forbidden: not usable by user or serviceaccount, provider "hostmount-anyuid": Forbidden: not usable by user or serviceaccount, provider "machine-api-termination-handler": Forbidden: not usable by user or serviceaccount, provider "hostnetwork-v2": Forbidden: not usable by user or serviceaccount, provider "hostnetwork": Forbidden: not usable by user or serviceaccount, provider "hostaccess": Forbidden: not usable by user or serviceaccount, provider "falco": Forbidden: not usable by user or serviceaccount, provider "node-exporter": Forbidden: not usable by user or serviceaccount, provider "privileged": Forbidden: not usable by user or serviceaccount]
6s          Warning   FailedCreate             replicaset/falco-falcosidekick-ui-76885bd484   Error creating: pods "falco-falcosidekick-ui-76885bd484-" is forbidden: unable to validate against any security context constraint: [provider "anyuid": Forbidden: not usable by user or serviceaccount, provider restricted-v2: .spec.securityContext.fsGroup: Invalid value: []int64{1234}: 1234 is not an allowed group, provider restricted-v2: .containers[0].runAsUser: Invalid value: 1234: must be in the ranges: [1000730000, 1000739999], provider "restricted": Forbidden: not usable by user or serviceaccount, provider "nonroot-v2": Forbidden: not usable by user or serviceaccount, provider "nonroot": Forbidden: not usable by user or serviceaccount, provider "hostmount-anyuid": Forbidden: not usable by user or serviceaccount, provider "machine-api-termination-handler": Forbidden: not usable by user or serviceaccount, provider "hostnetwork-v2": Forbidden: not usable by user or serviceaccount, provider "hostnetwork": Forbidden: not usable by user or serviceaccount, provider "hostaccess": Forbidden: not usable by user or serviceaccount, provider "falco": Forbidden: not usable by user or serviceaccount, provider "node-exporter": Forbidden: not usable by user or serviceaccount, provider "privileged": Forbidden: not usable by user or serviceaccount]
#+end_src

** Summary of problems

- The falco ~DaemonSet~ fails to start pods because there seems to be an issue with a missing directory
- Falco Sidekick and Falco Sidekick UI fail to start because of security context constraint issues

* Fixing the Falco daemonset

Falco tries to download a pre-compiled eBPF probe, fails and then
tries to compile that probe for our host OS kernel. This fails with

#+begin_src bash
make[1]: *** /lib/modules/4.18.0-372.73.1.el8_6.x86_64/build: No such file or directory.  Stop.
#+end_src

As far as we know there are no kernel sources on RHCOS nodes in
OpenShift.

After a little bit of searching the interweb we found the following issue comment on github:

https://github.com/falcosecurity/falco/issues/1505#issuecomment-754745960

So we need to enable the ~kernel-devel~ extension, the official docs are
[[https://docs.openshift.com/container-platform/4.12/post_installation_configuration/machine-configuration-tasks.html#rhcos-add-extensions_post-install-machine-configuration-tasks][here]]. It does not mention ~kernel-devel~, but there a [[https://access.redhat.com/solutions/6972423][knowledge base
article]] mentioning ~kernel-devel~, so let's give it a try.

We deploy two ~MachineConfigs~, one for [[https://github.com/tosmi-gitops/openshift-gitops/blob/main/components/apps/falco/base/worker-machineconfig.yaml][worker]] and one for [[https://github.com/tosmi-gitops/openshift-gitops/blob/main/components/apps/falco/base/master-machineconfig.yaml][master]] nodes
to rollout the extension:

#+begin_src yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 99-worker-kernel-devel-extensions
spec:
  extensions:
    - kernel-devel
#+end_src

See also our Kustomize configuration [[https://github.com/tosmi-gitops/openshift-gitops/blob/main/components/apps/falco/base/kustomization.yaml][here]].

As soon as we apply our ~MachineConfigs~, OpenShift starts the rollout:

#+begin_src
$ oc get mcp
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master   rendered-master-ce464ff45cc049fce3e8a63e36a4ee9e   False     True       False      3              0                   0                     0                      13d
worker   rendered-worker-a0f8f0d915ef01ba4a1ab3047b6c863d   False     True       False      3              0                   0                     0                      13d
#+end_src

When the rollout is done, let's restart all Falco ~DaemonSet~ pods:

#+begin_src bash
$ oc delete pods -l app.kubernetes.io/name=falco
#+end_src

And check the status:

#+begin_src bash
$ oc get pods
NAME                             READY   STATUS                  RESTARTS     AGE
falco-5wfnk                      0/2     Init:Error              1 (3s ago)   7s
falco-66fxw                      0/2     Init:0/2                1 (2s ago)   6s
falco-6fbc7                      0/2     Init:CrashLoopBackOff   1 (2s ago)   8s
falco-8h8n4                      0/2     Init:0/2                1 (2s ago)   6s
falco-falcosidekick-ui-redis-0   1/1     Running                 0            18m
falco-nhld2                      0/2     Init:CrashLoopBackOff   1 (2s ago)   6s
falco-xqv4b                      0/2     Init:CrashLoopBackOff   1 (3s ago)   8s
#+end_src

still, the ~initContainers~ fail. Lets check the log again

#+begin_src bash
$ oc logs -c falco-driver-loader falco-5wfnk
* Setting up /usr/src links from host
* Running falco-driver-loader for: falco version=0.36.1, driver version=6.0.1+driver, arch=x86_64, kernel release=4.18.0-372.73.1.el8_6.x86_64, kernel version=1
* Running falco-driver-loader with: driver=bpf, compile=yes, download=yes
* Mounting debugfs
mount: /sys/kernel/debug: permission denied.
       dmesg(1) may have more information after failed mount system call.
* Filename 'falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o' is composed of:
 - driver name: falco
 - target identifier: rhcos
 - kernel release: 4.18.0-372.73.1.el8_6.x86_64
 - kernel version: 1
* Trying to download a prebuilt eBPF probe from https://download.falco.org/driver/6.0.1%2Bdriver/x86_64/falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o
curl: (22) The requested URL returned error: 404
Unable to find a prebuilt falco eBPF probe
* Trying to compile the eBPF probe (falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o)
Makefile:1005: *** "Cannot generate ORC metadata for CONFIG_UNWINDER_ORC=y, please install libelf-dev, libelf-devel or elfutils-libelf-devel".  Stop.
make: *** [Makefile:38: all] Error 2
mv: cannot stat '/usr/src/falco-6.0.1+driver/bpf/probe.o': No such file or directory
Unable to load the falco eBPF probe
#+end_src

So this time we get another error, the culprit seems to be

#+begin_src bash
Makefile:1005: *** "Cannot generate ORC metadata for CONFIG_UNWINDER_ORC=y, please install libelf-dev, libelf-devel or elfutils-libelf-devel".  Stop.
#+end_src

Back to searching the interweb only reveals an old [[https://github.com/falcosecurity/falco/issues/376][issue]], that should
be fixed already.

So as a quick hack we [[https://github.com/tosmi/playground/blob/master/openshift/falco/custom-falco-driver-loader/Dockerfile][modified]] the ~falco-driver-loader~ image to
contain ~libelf-def~ and pushed to image to [[https://quay.io/repository/tosmi/falco-driver-loader?tab=tags][quay]].

We then modified our falco helm configuration to use the updated image:

#+begin_src yaml
driver:
  kind: ebpf
  loader:
    initContainer:
      image:
        registry: quay.io
        repository: tosmi/falco-driver-loader
        tag: 0.36.1-libelf-dev

falco:
  json_output: true
  json_include_output_property: true
  log_syslog: false
  log_level: info

falcosidekick:
  enabled: true
  webui:
    enabled: true
#+end_src

Note the updated ~diver.loader.initContainer~ section.

Let's check the our pods again:

#+begin_src shell
$ oc get pods
NAME                                  READY   STATUS    RESTARTS   AGE
falco-2ssgx                           2/2     Running   0          66s
falco-5hqgg                           1/2     Running   0          66s
falco-82kq9                           2/2     Running   0          65s
falco-99zxw                           2/2     Running   0          65s
falco-falcosidekick-test-connection   0/1     Error     0          67s
falco-falcosidekick-ui-redis-0        1/1     Running   0          31m
falco-slx5k                           2/2     Running   0          65s
falco-tzm8d                           2/2     Running   0          65s
#+end_src

Success! This time the ~DaemonSet~ pods started successfully. Just note
that you have to be patient. The first start took about 1-2 minutes to
complete.

Let's check the logs of one ~DaemonSet~ pod just to sure:

#+begin_src shell
oc logs -c falco-driver-loader falco-2ssgx
* Setting up /usr/src links from host
* Running falco-driver-loader for: falco version=0.36.1, driver version=6.0.1+driver, arch=x86_64, kernel release=4.18.0-372.73.1.el8_6.x86_64, kernel version=1
* Running falco-driver-loader with: driver=bpf, compile=yes, download=yes
* Mounting debugfs
mount: /sys/kernel/debug: permission denied.
       dmesg(1) may have more information after failed mount system call.
* Filename 'falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o' is composed of:
 - driver name: falco
 - target identifier: rhcos
 - kernel release: 4.18.0-372.73.1.el8_6.x86_64
 - kernel version: 1
* Trying to download a prebuilt eBPF probe from https://download.falco.org/driver/6.0.1%2Bdriver/x86_64/falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o
curl: (22) The requested URL returned error: 404
Unable to find a prebuilt falco eBPF probe
* Trying to compile the eBPF probe (falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o)
* eBPF probe located in /root/.falco/6.0.1+driver/x86_64/falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o
* Success: eBPF probe symlinked to /root/.falco/falco-bpf.o
#+end_src

Especially the line

#+begin_src
* Success: eBPF probe symlinked to /root/.falco/falco-bpf.o
#+end_src

looks promising. So up to the next problem, getting falco-sidekick and falco-sidekick-ui running.

* Fixing falco-sidekick and falco-sidekick-ui

Remember pod startup actually failed because of the following event (check with ~oc get events~):

#+begin_src
.spec.securityContext.fsGroup: Invalid value: []int64{1234}: 1234 is not an allowed group
#+end_src

It seems the sidekick pods want to run with a specific UID. The
default OpenShift Security Context Constraint (SCC) restricted
prohibits this.

Lets confirm our suspicion:

#+begin_src shell
$ oc get deploy -o jsonpath='{.spec.template.spec.securityContext}{"\n"}' falco-falcosidekick
{"fsGroup":1234,"runAsUser":1234}
$ oc get deploy -o jsonpath='{.spec.template.spec.securityContext}{"\n"}' falco-falcosidekick-ui
{"fsGroup":1234,"runAsUser":1234}
#+end_src

Bingo! ~securityContext~ is set to 1234 for both deployments. There is
another SCC that we could leverage, ~nonroot~ which basically allows any
UID expect 0. We just need to get the ~ServiceAccount~ that
falco-sidekick and falco-sidekick-ui are actually using:

#+begin_src shell
$ oc get deploy -o jsonpath='{.spec.template.spec.serviceAccount}{"\n"}' falco-falcosidekick
falco-falcosidekick
$ oc get deploy -o jsonpath='{.spec.template.spec.serviceAccount}{"\n"}' falco-falcosidekick-ui
falco-falcosidekick-ui
#+end_src

So falco-sidekick uses ~falco-sidekick~ as ~ServiceAccount~ and falco-sidekick-ui ~falco-sidekick-ui~. Lets
grant both ~ServiceAccount~ access to the ~nonroot~ SCC.

#+begin_src
kind: ClusterRoleBinding
metadata:
  name: falco-falcosidekick-scc:nonroot
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:openshift:scc:nonroot
subjects:
- kind: ServiceAccount
  name: falco-falcosidekick
  namespace: falco
- kind: ServiceAccount
  name: falco-falcosidekick-ui
  namespace: falco
#+end_src

We've already added this [[https://github.com/tosmi-gitops/openshift-gitops/blob/main/components/apps/falco/base/falcosidekick-any-uid-scc.yaml][file]] to our [[https://github.com/tosmi-gitops/openshift-gitops/blob/main/components/apps/falco/base/falcosidekick-any-uid-scc.yaml#L19][Kustomize]] configuration.

Let's trigger a redeployment by deleting the ~ReplicaSets~ of both deployments, they will be re-created automatically:

#+begin_src shell
$ oc delete rs -l app.kubernetes.io/name=falcosidekick
$ oc delete rs -l app.kubernetes.io/name=falcosidekick-ui
#+end_src

Finally let's confirm everything is up and running:

#+begin_src shell
$ oc get deploy,ds,pods
NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/falco-falcosidekick      2/2     2            2           5d
deployment.apps/falco-falcosidekick-ui   2/2     2            2           5d

NAME                   DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/falco   6         6         6       6            6           <none>          6d2h

NAME                                          READY   STATUS    RESTARTS   AGE
pod/falco-2ssgx                               2/2     Running   0          21m
pod/falco-5hqgg                               2/2     Running   0          21m
pod/falco-82kq9                               2/2     Running   0          21m
pod/falco-99zxw                               2/2     Running   0          21m
pod/falco-falcosidekick-7cfbbbf89f-qxwxs      1/1     Running   0          118s
pod/falco-falcosidekick-7cfbbbf89f-rz5lj      1/1     Running   0          118s
pod/falco-falcosidekick-ui-76885bd484-p7lqm   1/1     Running   0          2m18s
pod/falco-falcosidekick-ui-76885bd484-sfgh4   1/1     Running   0          2m18s
pod/falco-falcosidekick-ui-redis-0            1/1     Running   0          51m
pod/falco-slx5k                               2/2     Running   0          21m
pod/falco-tzm8d                               2/2     Running   0          21m
#+end_src

* Testing Falco

Now that everything seems to be up and running, lets do a quick
test. First we will try to access the Falco Sidekick user interface.

Falco will not deploy a route for the UI automatically, instead we've
created a [[https://github.com/tosmi-gitops/openshift-gitops/tree/main/components/apps/falco/overlays/sidekick-ui-route][Kustomize overlay]] with a custom route:

#+begin_src yaml
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: falco-falcosidekick-ui
  namespace: falco
spec:
  host: falcosidekick-ui.apps.hub.aws.tntinfra.net
  port:
    targetPort: http
  tls:
    termination: edge
  to:
    kind: Service
    name: falco-falcosidekick-ui
  wildcardPolicy: None
#+end_src

After deploying the ~Route~ we can access the Falco UI with the hostname
specified in the route object. The default username seems to be
_admin/admin_ which is strange for a security tool, maybe that's the
reason Falco does not expose the UI per default.

[[file:/OpenShift/images/falco/falco-ui.png]]

** Create an alert

As a last test let's try to trigger an alert. We open a shell to one
of the falco ~DaemonSet~ pods and execute a suspicious command:

#+begin_src shell
$ oc rsh falco-2ssgx
Defaulted container "falco" out of: falco, falcoctl-artifact-follow, falco-driver-loader (init), falcoctl-artif# cat /etc/shadow
root:*:19639:0:99999:7:::
daemon:*:19639:0:99999:7:::
bin:*:19639:0:99999:7:::
sys:*:19639:0:99999:7:::
sync:*:19639:0:99999:7:::
games:*:19639:0:99999:7:::
man:*:19639:0:99999:7:::
lp:*:19639:0:99999:7:::
mail:*:19639:0:99999:7:::
news:*:19639:0:99999:7:::
uucp:*:19639:0:99999:7:::
proxy:*:19639:0:99999:7:::
www-data:*:19639:0:99999:7:::
backup:*:19639:0:99999:7:::
list:*:19639:0:99999:7:::
irc:*:19639:0:99999:7:::
_apt:*:19639:0:99999:7:::
nobody:*:19639:0:99999:7:::
#
#+end_src

and we can see an event with priority *Warning* in the Falco ui.

[[file:/OpenShift/images/falco/falco-cat-etc-shadow.png]]

That's it, seems like Falco is successfully running on OpenShift 4.12.
