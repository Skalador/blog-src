--- 
title: "Pod Placement - NodeSelector"
description: "Placeing Pods Using the NodeSelector"
date: "2021-08-27"
doctype: book

author: Thomas Jungbauer
type: post
draft: true
categories:
   - OpenShift
   - Day-2
   - Pod Placement
   - nodeSelector
tags: ["OCP", "Day-2", "OpenShift", "Pod Placement", "NodeSelector"] 

aliases: [ 
	 "/openshift/day-2/pod-placement-nodeselector",
] 
---

:imagesdir: /OpenShift/Day-2/images/
:icons: font
:toc:

One of the easiest ways to tell your Kubernetes cluster where to put certain pods is to use a `nodeSelector` specification. A nodeSelector defines a key-value pair. 
The selectors are defined inside the specification of the pods and as a label on one or multiple nodes (or machine set or machine config). Only if selector matches the node label, the pod is allowed to be scheduled on that node. 

<!--more--> 

Kubernetes distingushes between 2 types of selectors: 

. _cluster-wide node selectors_: defined by the cluster administrators and valid for the whole cluster
. _project node selectors_: to place new pods inside projects into specific nodes.

Here we will descuss the project node selectors. 

== Using nodeSelector

As previously described, we have a cluster with an example application running, running accross the worker nodes evenly by the scheduler. 

[source,bash]
----
oc get pods -n podtesting -o wide | grep Running

django-psql-example-1-842fl    1/1     Running             0          2m7s   10.131.0.65   compute-3   <none>           <none>
django-psql-example-1-h6kst    1/1     Running             0          24m    10.130.2.97   compute-2   <none>           <none>
django-psql-example-1-pxhlv    1/1     Running             0          2m7s   10.128.2.13   compute-0   <none>           <none>
django-psql-example-1-xms7x    1/1     Running             0          2m7s   10.129.2.10   compute-1   <none>           <none>
postgresql-1-4pcm4             1/1     Running             0          26m    10.131.0.51   compute-3   <none>           <none>
----

However, our 4 compute nodes are using different harddisks (sdd vs hdd) and our web application must run on nodes with SSD only. 

To start using nodeSelectors we first label our nodes accordingly: 

* compute-0 and compute-1 are faster nodes with an SSD attached.
* compute-2 and compute-2 have a HDD attached. 

[source,bash]
----
oc label nodes compute-0 compute-1 disktype=ssd <1>

oc label nodes compute-2 compute-3 disktype=hdd
----
<1> as key we are using *disktype*

As crosscheck we can list nodes with a specific label: 

[source,bash]
----
oc get nodes -l disktype=ssd
NAME        STATUS   ROLES    AGE     VERSION
compute-0   Ready    worker   7h32m   v1.19.0+d59ce34
compute-1   Ready    worker   7h31m   v1.19.0+d59ce34

oc get nodes -l disktype=hdd
NAME        STATUS   ROLES    AGE     VERSION
compute-2   Ready    worker   7h32m   v1.19.0+d59ce34
compute-3   Ready    worker   7h32m   v1.19.0+d59ce34
----

WARNING: If no matching label is found, the pod cannot be scheduled. Therefore, *always* label the nodes first.

The 2nd step is to add the node selector to the specification of the pod. In our example we are using a DeploymentConfig, so let's add it there: 

[source,bash]
----
oc patch dc django-psql-example -n podtesting --patch '{"spec":{"template":{"spec":{"nodeSelector":{"disktype":"ssd"}}}}}'
----

This adds the nodeSelector into: spec/template/spec 

[source,yaml]
----
       nodeSelector:
         disktype: ssd
----

Kubernetes will now trigger a restart of the pods on the supposed nodes. 

[source,bash]
----
oc get pods -n podtesting -o wide | grep Running

django-psql-example-3-4j92k    1/1     Running       0          42s   10.129.2.7    compute-1   <none>           <none>
django-psql-example-3-d7hsd    1/1     Running       0          42s   10.129.2.8    compute-1   <none>           <none>
django-psql-example-3-fkbfm    1/1     Running       0          14m   10.128.2.18   compute-0   <none>           <none>
django-psql-example-3-psskb    1/1     Running       0          14m   10.128.2.17   compute-0   <none>           <none>
----

== Controlling pod placement with project-wide selector 

Adding a nodeSelector to a deployment seems fine... until somebody forgets to add it. Therefore, it might make sense to use a project-wide node selector, which will automatically be applied on all pods on that project. The project selector is added to the *Namespace* (no matter what the OpenShift documentation says in it's example :-) ) object as `openshift.io/node-selector` parameter. 

Let's remove our previous configuration and add the setting to our namespace _podtesting_: 

. Cleanup
+
Remove the nodeSelector from the deployment configuration and wait until all pods have been reshuffeld 
+
[source,bash]
----
oc patch dc django-psql-example -n podtesting --type='json' -p='[{"op": "remove", "path": "/spec/template/spec/nodeSelector", "value": "disktype=ssd" }]'
----

. Add the label to the project
+
[source,bash]
----
oc annotate ns/podtesting openshift.io/node-selector="disktype=ssd"
----

The OpenShift scheduler will now spread the Pods accross compute-0 or compute-1 again, but not on compute-2 or 3.

We can prove that by stressing our cluster (and nodes) a little bit and scale our frontend application to 10: 

[source,bash]
----
oc get pods -n podtesting -o wide | grep Running
django-psql-example-4-2jn2l    1/1     Running     0          27s     10.128.2.8    compute-0   <none>           <none>
django-psql-example-4-6g7ks    1/1     Running     0          7m47s   10.129.2.23   compute-1   <none>           <none>
django-psql-example-4-752nm    1/1     Running     0          7m47s   10.128.2.7    compute-0   <none>           <none>
django-psql-example-4-c5jvm    1/1     Running     0          27s     10.129.2.4    compute-1   <none>           <none>
django-psql-example-4-f5kwg    1/1     Running     0          27s     10.129.2.5    compute-1   <none>           <none>
django-psql-example-4-g7bcs    1/1     Running     0          7m47s   10.129.2.24   compute-1   <none>           <none>
django-psql-example-4-h5tgb    1/1     Running     0          27s     10.129.2.6    compute-1   <none>           <none>
django-psql-example-4-spvpp    1/1     Running     0          28s     10.128.2.5    compute-0   <none>           <none>
django-psql-example-4-v9qwj    1/1     Running     0          7m48s   10.129.2.22   compute-1   <none>           <none>
django-psql-example-4-zgwcv    1/1     Running     0          27s     10.128.2.6    compute-0   <none>           <none>
----

As you can see compute-0 and compute-1 are the only onces which are used. 


== Well-Known Labels

nodeSelector is one of the easiest way to control where an application shall be started. Working with labels is therefore very important as soon as workload shall be added to the cluster. 
Kubernetes reserves some labels which can be leveraged and some are already predefined on the nodes, for example: 

* beta.kubernetes.io/arch=amd64
* kubernetes.io/hostname=compute-0
* kubernetes.io/os=linux
* node-role.kubernetes.io/worker=
* node.openshift.io/os_id=rhcos

A list of all known can be found at: [<<source_1,1>>]

Two of them I would like to mention, since they might become very important when designing the placement of pods: 

* topology.kubernetes.io/zone == a logical failure domain.
* topology.kubernetes.io/region == larger domain, cotaining one or more zones.

== Cleanup 

For the next chapter of the Pod Placement Series we need to cleanup our configuration. 

. Scale the frontend down to 2
+
[source,bash]
----
oc scale --replicas=2 dc/django-psql-example -n podtesting
----

. Remove the label from the namespace
+
[source,bash]
----
oc annotate ns/podtesting openshift.io/node-selector- <1>
----
<1> The minus at the end defines that this annotation shall be removed

== Sources
* [[source_1]][1]: https://kubernetes.io/docs/reference/labels-annotations-taints/[Well-Known Labels, Annotations and Taints^]